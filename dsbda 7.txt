import nltk

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

text= "Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization."

from nltk.tokenize import sent_tokenize
tokenized_text= sent_tokenize(text)
print(tokenized_text)
#Word Tokenization
from nltk.tokenize import word_tokenize
tokenized_word=word_tokenize(text)
print(tokenized_word)


# print stop words of English
import re
from nltk.corpus import stopwords
stop_words=set(stopwords.words("english"))
print(stop_words)
text= "How to remove stop words with NLTK library in Python?"
text= re.sub('[^a-zA-Z]', ' ',text)
tokens = word_tokenize(text.lower())
filtered_text=[]
for w in tokens:
    if w not in stop_words:
        filtered_text.append(w)
print("Tokenized Sentence:",tokens)
print("Filterd Sentence:",filtered_text)


from nltk.stem import PorterStemmer
e_words= ["wait", "waiting", "waited", "waits"]
ps =PorterStemmer()
for w in e_words:
   rootWord=ps.stem(w)
print(rootWord)


from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
text = "studies studying cries cry"
tokenization = nltk.word_tokenize(text)
for w in tokenization:
    print("Lemma for {} is {}".format(w,wordnet_lemmatizer.lemmatize(w)))


import nltk
from nltk.tokenize import word_tokenize
data="The pink sweater fit her perfectly"
words=word_tokenize(data)
for word in words:
   print(nltk.pos_tag([word]))



doc_a = 'Jupiter is the largest Planet'
doc_b = 'Mars is the fourth planet from the Sun'


bag_of_words_a = doc_a.split(' ')
bag_of_words_b = doc_b.split(' ')


unique_words_set = set(bag_of_words_a).union(set(bag_of_words_b))
print(unique_words_set) 
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer




*dict_a = dict.fromkeys(unique_words_set, 0)
# print(dict_a) # {'this': 0, 'document': 0, 'second': 0, 'is': 0, 'the': 0}

for word in bag_of_words_a:
    dict_a[word] += 1

print(dict_a)
# {'this': 1, 'document': 2, 'second': 1, 'is': 1, 'the': 1}

# similarly

dict_b = dict.fromkeys(unique_words_set, 0)
for word in bag_of_words_b:
    dict_b[word] += 1

print(dict_b)  *



def compute_term_frequency(word_dictionary, bag_of_words):
    term_frequency_dictionary = {}
    length_of_bag_of_words = len(bag_of_words)

    for word, count in word_dictionary.items():
        term_frequency_dictionary[word] = count / float(length_of_bag_of_words)

    return term_frequency_dictionary



# Implementation
print(compute_term_frequency(dict_a, bag_of_words_a))
print(compute_term_frequency(dict_b, bag_of_words_b))



import math
def compute_inverse_document_frequency(full_doc_list):
    idf_dict = {}
    length_of_doc_list = len(full_doc_list)

    idf_dict = dict.fromkeys(full_doc_list[0].keys(), 0)
    for word, value in idf_dict.items():
        idf_dict[word] = math.log(length_of_doc_list / (float(value) + 1))

    return idf_dict

final_idf_dict = compute_inverse_document_frequency([dict_a, dict_b])
print(final_idf_dict)



*def compute_term_frequency_inverse_document_frequency(bag_of_words, final_idf_dict): 
    tfidf = {} 
    for word, val in bag_of_words.items(): 
        tfidf[word] = val * final_idf_dict[word] 
        return tfidf

tfidfdict_a = compute_term_frequency_inverse_document_frequency(dict_a, final_idf_dict) 
tfidfdict_b = compute_term_frequency_inverse_document_frequency(dict_b, final_idf_dict)

print(tfidfdict_a) 
print(tfidfdict_b)
df = pd.DataFrame([tfidfdict_a,tfidfdict_b])
df*







